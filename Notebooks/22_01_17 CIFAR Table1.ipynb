{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR Table 1\n",
    "\n",
    "TJ Kim\n",
    "\n",
    "1.17.22\n",
    "\n",
    "#### Summary:\n",
    "- Make a table for Benign transferability and inter-boundary distance for following models\n",
    "- Local benign, fedavg benign, fedEM benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/FedEM\n"
     ]
    }
   ],
   "source": [
    "cd /home/ubuntu/FedEM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import General Libraries\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import FedEM based Libraries\n",
    "from utils.utils import *\n",
    "from utils.constants import *\n",
    "from utils.args import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from run_experiment import *\n",
    "from models import *\n",
    "\n",
    "# Import Transfer Attack\n",
    "from transfer_attacks.Personalized_NN import *\n",
    "from transfer_attacks.Params import *\n",
    "from transfer_attacks.Transferer import *\n",
    "from transfer_attacks.Args import *\n",
    "\n",
    "from transfer_attacks.TA_utils import *\n",
    "from transfer_attacks.Boundary_Transferer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:00<00:00, 265.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:09<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Test Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++\n",
      "Global..\n",
      "Train Loss: 2.297 | Train Acc: 10.823% |Test Loss: 2.298 | Test Acc: 10.893% |\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "# Manually set argument parameters\n",
    "args_ = Args()\n",
    "args_.experiment = \"cifar10\"\n",
    "args_.method = \"FedAvg\"\n",
    "args_.decentralized = False\n",
    "args_.sampling_rate = 1.0\n",
    "args_.input_dimension = None\n",
    "args_.output_dimension = None\n",
    "args_.n_learners= 1\n",
    "args_.n_rounds = 10\n",
    "args_.bz = 128\n",
    "args_.local_steps = 1\n",
    "args_.lr_lambda = 0\n",
    "args_.lr =0.03\n",
    "args_.lr_scheduler = 'multi_step'\n",
    "args_.log_freq = 10\n",
    "args_.device = 'cuda'\n",
    "args_.optimizer = 'sgd'\n",
    "args_.mu = 0\n",
    "args_.communication_probability = 0.1\n",
    "args_.q = 1\n",
    "args_.locally_tune_clients = False\n",
    "args_.seed = 1234\n",
    "args_.verbose = 1\n",
    "args_.save_path = 'weights/cifar/dummy/'\n",
    "args_.validation = False\n",
    "\n",
    "# Generate the dummy values here\n",
    "aggregator, clients = dummy_aggregator(args_, num_user=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Dataset to be used throughout all analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Validation Data across all clients as test\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for i in range(len(clients)):\n",
    "    daniloader = clients[i].val_iterator\n",
    "    for (x,y,idx) in daniloader.dataset:\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "\n",
    "data_x = torch.stack(data_x)\n",
    "data_y = torch.stack(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader from validation dataset that allows for diverse batch size\n",
    "dataloader = Custom_Dataloader(data_x, data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load local model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = 'FedAvg'\n",
    "num_models = 8\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "\n",
    "if setting == 'local':\n",
    "\n",
    "    # args_.save_path = 'weights/final/cifar/fig1/Local_Benign/'\n",
    "    args_.save_path ='weights/final/cifar/fig3/local_defend'\n",
    "    aggregator.load_state(args_.save_path)\n",
    "    \n",
    "    model_weights = []\n",
    "    # weights = np.load(\"weights/final/cifar/fig1/Local_Benign/train_client_weights.npy\")\n",
    "    weights = np.load('weights/final/cifar/fig3/local_defend/train_client_weights.npy')\n",
    "    \n",
    "    for i in range(num_models):\n",
    "        model_weights += [weights[i]]\n",
    "\n",
    "    # Generate the weights to test on as linear combinations of the model_weights\n",
    "    models_test = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        new_model = copy.deepcopy(aggregator.clients[i].learners_ensemble.learners[0].model)\n",
    "        new_model.eval()\n",
    "        models_test += [new_model]\n",
    "\n",
    "elif setting == 'FedAvg':\n",
    "    \n",
    "#     args_.save_path = 'weights/final/cifar/fig1/FedAvg_Benign/'\n",
    "    args_.save_path = 'weights/final/cifar/fig3/fed_avg_defend'\n",
    "    aggregator.load_state(args_.save_path)\n",
    "    \n",
    "    # This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "    hypotheses = aggregator.global_learners_ensemble.learners\n",
    "\n",
    "    # obtain the state dict for each of the weights \n",
    "    weights_h = []\n",
    "\n",
    "    for h in hypotheses:\n",
    "        weights_h += [h.model.state_dict()]\n",
    "\n",
    "    # weights = np.load(\"weights/final/cifar/fig1/FedAvg_Benign/train_client_weights.npy\")\n",
    "    weights = np.load('weights/final/cifar/fig3/fed_avg_defend/train_client_weights.npy')\n",
    "    \n",
    "    # Set model weights\n",
    "    model_weights = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        model_weights += [weights[i]]\n",
    "\n",
    "    # Generate the weights to test on as linear combinations of the model_weights\n",
    "    models_test = []\n",
    "\n",
    "    for (w0) in model_weights:\n",
    "        # first make the model with empty weights\n",
    "        new_model = copy.deepcopy(hypotheses[0].model)\n",
    "        new_model.eval()\n",
    "        new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "        for key in weights_h[0]:\n",
    "            new_weight_dict[key] = w0[0]*weights_h[0][key] \n",
    "        new_model.load_state_dict(new_weight_dict)\n",
    "        models_test += [new_model]\n",
    "\n",
    "elif setting == 'FedEM':\n",
    "    \n",
    "    # args_.save_path = 'weights/cifar/21_12_30_feddef_n40_linf0_5_G0_0/'\n",
    "    args_.save_path = 'weights/final/cifar/fig3/fedem_defend/'\n",
    "    aggregator.load_state(args_.save_path)\n",
    "    \n",
    "    # This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "    hypotheses = aggregator.global_learners_ensemble.learners\n",
    "\n",
    "    # obtain the state dict for each of the weights \n",
    "    weights_h = []\n",
    "\n",
    "    for h in hypotheses:\n",
    "        weights_h += [h.model.state_dict()]\n",
    "\n",
    "#     weights = np.load(\"weights/cifar/21_12_30_feddef_n40_linf0_5_G0_0/train_client_weights.npy\")\n",
    "    weights = np.load(\"weights/final/cifar/fig3/fedem_defend/train_client_weights.npy\")\n",
    "\n",
    "    # Set model weights\n",
    "    model_weights = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        model_weights += [weights[i]]\n",
    "\n",
    "\n",
    "    # Generate the weights to test on as linear combinations of the model_weights\n",
    "    models_test = []\n",
    "\n",
    "    for (w0,w1,w2) in model_weights:\n",
    "        # first make the model with empty weights\n",
    "        new_model = copy.deepcopy(hypotheses[0].model)\n",
    "        new_model.eval()\n",
    "        new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "        for key in weights_h[0]:\n",
    "            new_weight_dict[key] = w0*weights_h[0][key] + w1*weights_h[1][key] + w2*weights_h[2][key]\n",
    "        new_model.load_state_dict(new_weight_dict)\n",
    "        models_test += [new_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up Transfer Attack Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_adv = []\n",
    "\n",
    "for i in range(num_models):\n",
    "    adv_dict = {}\n",
    "    adv_dict['orig_acc_transfers'] = None\n",
    "    adv_dict['orig_similarities'] = None\n",
    "    adv_dict['adv_acc_transfers'] = None\n",
    "    adv_dict['adv_similarities_target'] = None\n",
    "    adv_dict['adv_similarities_untarget'] = None\n",
    "    adv_dict['adv_target'] = None\n",
    "    adv_dict['adv_miss'] = None\n",
    "    adv_dict['metric_alignment'] = None\n",
    "    adv_dict['ib_distance_legit'] = None\n",
    "    adv_dict['ib_distance_adv'] = None\n",
    "\n",
    "    logs_adv += [adv_dict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Adv idx: 0\n",
      "\t Adv idx: 1\n",
      "\t Adv idx: 2\n",
      "\t Adv idx: 3\n",
      "\t Adv idx: 4\n",
      "\t Adv idx: 5\n",
      "\t Adv idx: 6\n",
      "\t Adv idx: 7\n"
     ]
    }
   ],
   "source": [
    "# Run Measurements for both targetted and untargeted analysis\n",
    "victim_idxs = range(num_models)\n",
    "\n",
    "t1 = Transferer(models_list=models_test, dataloader=dataloader)\n",
    "t1.generate_victims(victim_idxs)\n",
    "\n",
    "for adv_idx in victim_idxs:\n",
    "    print(\"\\t Adv idx:\", adv_idx)\n",
    "    # Perform Attacks\n",
    "    t1.atk_params = PGD_Params()\n",
    "    t1.atk_params.set_params(batch_size=500, iteration = 20,\n",
    "                   target = 3, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x),\n",
    "                   step_size = 0.05, step_norm = \"inf\", eps = 4.5, eps_norm = 2)\n",
    "\n",
    "    t1.generate_advNN(adv_idx)\n",
    "    t1.generate_xadv(atk_type = \"pgd\")\n",
    "    t1.send_to_victims(victim_idxs)\n",
    "\n",
    "    # Log Performance\n",
    "    logs_adv[adv_idx]['orig_acc_transfers'] = copy.deepcopy(t1.orig_acc_transfers)\n",
    "    logs_adv[adv_idx]['orig_similarities'] = copy.deepcopy(t1.orig_similarities)\n",
    "    logs_adv[adv_idx]['adv_acc_transfers'] = copy.deepcopy(t1.adv_acc_transfers)\n",
    "    logs_adv[adv_idx]['adv_similarities_target'] = copy.deepcopy(t1.adv_similarities)        \n",
    "    logs_adv[adv_idx]['adv_target'] = copy.deepcopy(t1.adv_target_hit)\n",
    "\n",
    "    # Miss attack\n",
    "    t1.atk_params.set_params(batch_size=500, iteration = 20,\n",
    "                   target = -1, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x),\n",
    "                   step_size = 0.05, step_norm = \"inf\", eps = 4.5, eps_norm = 2)\n",
    "    t1.generate_xadv(atk_type = \"pgd\")\n",
    "    t1.send_to_victims(victim_idxs)\n",
    "    logs_adv[adv_idx]['adv_miss'] = copy.deepcopy(t1.adv_acc_transfers)\n",
    "    logs_adv[adv_idx]['adv_similarities_untarget'] = copy.deepcopy(t1.adv_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Relevant Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['orig_acc_transfers','orig_similarities','adv_acc_transfers','adv_similarities_target',\n",
    "           'adv_similarities_untarget','adv_target','adv_miss'] #,'metric_alignment']\n",
    "\n",
    "orig_acc = np.zeros([len(victim_idxs),len(victim_idxs)]) \n",
    "orig_sim = np.zeros([len(victim_idxs),len(victim_idxs)]) \n",
    "adv_acc = np.zeros([len(victim_idxs),len(victim_idxs)]) \n",
    "adv_sim_target = np.zeros([len(victim_idxs),len(victim_idxs)]) \n",
    "adv_sim_untarget = np.zeros([len(victim_idxs),len(victim_idxs)]) \n",
    "adv_target = np.zeros([len(victim_idxs),len(victim_idxs)])\n",
    "adv_miss = np.zeros([len(victim_idxs),len(victim_idxs)]) \n",
    "\n",
    "for adv_idx in range(len(victim_idxs)):\n",
    "    for victim in range(len(victim_idxs)):\n",
    "        orig_acc[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[0]][victim_idxs[victim]].data.tolist()\n",
    "        orig_sim[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[1]][victim_idxs[victim]].data.tolist()\n",
    "        adv_acc[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[2]][victim_idxs[victim]].data.tolist()\n",
    "        adv_sim_target[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[3]][victim_idxs[victim]].data.tolist()\n",
    "        adv_sim_untarget[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[4]][victim_idxs[victim]].data.tolist()\n",
    "        adv_target[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[5]][victim_idxs[victim]].data.tolist()\n",
    "        adv_miss[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[6]][victim_idxs[victim]].data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21798112988471985"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_nondiag(adv_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up Interboundary Measure Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trial: 0\n",
      "num_trial: 1\n",
      "num_trial: 2\n",
      "num_trial: 3\n",
      "num_trial: 4\n",
      "num_trial: 5\n",
      "num_trial: 6\n",
      "num_trial: 7\n",
      "num_trial: 8\n",
      "num_trial: 9\n",
      "num_trial: 10\n",
      "num_trial: 11\n",
      "num_trial: 12\n",
      "num_trial: 13\n",
      "num_trial: 14\n",
      "num_trial: 15\n",
      "num_trial: 16\n",
      "num_trial: 17\n",
      "num_trial: 18\n",
      "num_trial: 19\n",
      "num_trial: 20\n",
      "num_trial: 21\n",
      "num_trial: 22\n",
      "num_trial: 23\n",
      "num_trial: 24\n",
      "num_trial: 25\n",
      "num_trial: 26\n",
      "num_trial: 27\n",
      "num_trial: 28\n",
      "num_trial: 29\n",
      "num_trial: 30\n",
      "num_trial: 31\n",
      "num_trial: 32\n",
      "num_trial: 33\n",
      "num_trial: 34\n",
      "num_trial: 35\n",
      "num_trial: 36\n",
      "num_trial: 37\n",
      "num_trial: 38\n",
      "num_trial: 39\n",
      "num_trial: 40\n",
      "num_trial: 41\n",
      "num_trial: 42\n",
      "num_trial: 43\n",
      "num_trial: 44\n",
      "num_trial: 45\n",
      "num_trial: 46\n",
      "num_trial: 47\n",
      "num_trial: 48\n",
      "num_trial: 49\n",
      "num_trial: 50\n",
      "num_trial: 51\n",
      "num_trial: 52\n",
      "num_trial: 53\n",
      "num_trial: 54\n",
      "num_trial: 55\n",
      "num_trial: 56\n",
      "num_trial: 57\n",
      "num_trial: 58\n",
      "num_trial: 59\n",
      "num_trial: 60\n",
      "num_trial: 61\n",
      "num_trial: 62\n",
      "num_trial: 63\n",
      "num_trial: 64\n",
      "num_trial: 65\n",
      "num_trial: 66\n",
      "num_trial: 67\n",
      "num_trial: 68\n",
      "num_trial: 69\n",
      "num_trial: 70\n",
      "num_trial: 71\n",
      "num_trial: 72\n",
      "num_trial: 73\n",
      "num_trial: 74\n",
      "num_trial: 75\n",
      "num_trial: 76\n",
      "num_trial: 77\n",
      "num_trial: 78\n",
      "num_trial: 79\n",
      "num_trial: 80\n",
      "num_trial: 81\n",
      "num_trial: 82\n",
      "num_trial: 83\n",
      "num_trial: 84\n",
      "num_trial: 85\n",
      "num_trial: 86\n",
      "num_trial: 87\n",
      "num_trial: 88\n",
      "num_trial: 89\n",
      "num_trial: 90\n",
      "num_trial: 91\n",
      "num_trial: 92\n",
      "num_trial: 93\n",
      "num_trial: 94\n",
      "num_trial: 95\n",
      "num_trial: 96\n",
      "num_trial: 97\n",
      "num_trial: 98\n",
      "num_trial: 99\n",
      "num_trial: 100\n",
      "num_trial: 101\n",
      "num_trial: 102\n",
      "num_trial: 103\n",
      "num_trial: 104\n",
      "num_trial: 105\n",
      "num_trial: 106\n",
      "num_trial: 107\n",
      "num_trial: 108\n",
      "num_trial: 109\n",
      "num_trial: 110\n",
      "num_trial: 111\n",
      "num_trial: 112\n",
      "num_trial: 113\n",
      "num_trial: 114\n",
      "num_trial: 115\n",
      "num_trial: 116\n",
      "num_trial: 117\n",
      "num_trial: 118\n",
      "num_trial: 119\n",
      "num_trial: 120\n",
      "num_trial: 121\n",
      "num_trial: 122\n",
      "num_trial: 123\n",
      "num_trial: 124\n",
      "num_trial: 125\n",
      "num_trial: 126\n",
      "num_trial: 127\n",
      "num_trial: 128\n",
      "num_trial: 129\n",
      "num_trial: 130\n",
      "num_trial: 131\n",
      "num_trial: 132\n",
      "num_trial: 133\n",
      "num_trial: 134\n",
      "num_trial: 135\n",
      "num_trial: 136\n",
      "num_trial: 137\n",
      "num_trial: 138\n",
      "num_trial: 139\n",
      "num_trial: 140\n",
      "num_trial: 141\n",
      "num_trial: 142\n",
      "num_trial: 143\n",
      "num_trial: 144\n",
      "num_trial: 145\n",
      "num_trial: 146\n",
      "num_trial: 147\n",
      "num_trial: 148\n",
      "num_trial: 149\n",
      "num_trial: 150\n",
      "num_trial: 151\n",
      "num_trial: 152\n",
      "num_trial: 153\n",
      "num_trial: 154\n",
      "num_trial: 155\n",
      "num_trial: 156\n",
      "num_trial: 157\n",
      "num_trial: 158\n",
      "num_trial: 159\n",
      "num_trial: 160\n",
      "num_trial: 161\n",
      "num_trial: 162\n",
      "num_trial: 163\n",
      "num_trial: 164\n",
      "num_trial: 165\n",
      "num_trial: 166\n",
      "num_trial: 167\n",
      "num_trial: 168\n",
      "num_trial: 169\n",
      "num_trial: 170\n",
      "num_trial: 171\n",
      "num_trial: 172\n",
      "num_trial: 173\n",
      "num_trial: 174\n",
      "num_trial: 175\n",
      "num_trial: 176\n",
      "num_trial: 177\n",
      "num_trial: 178\n",
      "num_trial: 179\n",
      "num_trial: 180\n",
      "num_trial: 181\n",
      "num_trial: 182\n",
      "num_trial: 183\n",
      "num_trial: 184\n",
      "num_trial: 185\n",
      "num_trial: 186\n",
      "num_trial: 187\n",
      "num_trial: 188\n",
      "num_trial: 189\n",
      "num_trial: 190\n",
      "num_trial: 191\n",
      "num_trial: 192\n",
      "num_trial: 193\n",
      "num_trial: 194\n",
      "num_trial: 195\n",
      "num_trial: 196\n",
      "num_trial: 197\n",
      "num_trial: 198\n",
      "num_trial: 199\n"
     ]
    }
   ],
   "source": [
    "num_trials = 200\n",
    "batch_size = 5000\n",
    "adv_idx = [0]\n",
    "\n",
    "t1 = Boundary_Transferer(models_list=models_test, dataloader=dataloader)\n",
    "t1.base_nn_idx = 0\n",
    "t1.victim_idx = [1,2,3,4]\n",
    "\n",
    "\n",
    "dists_measure_legit = np.zeros([num_trials, len(t1.victim_idx)])\n",
    "dists_measure_adv = np.zeros([num_trials, len(t1.victim_idx)])\n",
    "dists_measure_adv_ensemble = np.zeros([num_trials, len(t1.victim_idx)])\n",
    "\n",
    "for i in range(num_trials):\n",
    "    print(\"num_trial:\", i)\n",
    "    t1 = Boundary_Transferer(models_list=models_test, dataloader=dataloader)\n",
    "    t1.base_nn_idx = 0\n",
    "    t1.victim_idx = [1,2,3,4]\n",
    "\n",
    "    t1.atk_params = PGD_Params()\n",
    "    t1.atk_params.set_params(batch_size=500, iteration = 30,\n",
    "                   target = -1, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x),\n",
    "                   step_size = 0.05, step_norm = \"inf\", eps = 3, eps_norm = 2)\n",
    "    t1.set_adv_NN(t1.base_nn_idx)\n",
    "\n",
    "    base_ep_legit, victim_eps_legit = t1.legitimate_direction(batch_size=batch_size, ep_granularity = 0.5, \n",
    "                                                              rep_padding = 1000, new_point = True,print_res = False)\n",
    "    \n",
    "    base_ep_adv, victim_eps_adv = t1.adversarial_direction(ep_granularity = 0.5, \n",
    "                                                              rep_padding = 1000, new_point = False,print_res = False)\n",
    "    \n",
    "    idx = 0\n",
    "    for key, value in victim_eps_legit.items():\n",
    "        dists_measure_legit[i,idx] = np.abs(base_ep_legit-value)\n",
    "        idx+=1\n",
    "        \n",
    "    idx = 0\n",
    "    for key, value in victim_eps_adv.items():\n",
    "        dists_measure_adv[i,idx] = np.abs(base_ep_adv - value)\n",
    "        idx+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.214375000000004"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.average(dists_measure_legit,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.08125"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.average(dists_measure_adv,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local - Benign\n",
    "local_adv_miss = 0.37\n",
    "local_adv_target = 0.07\n",
    "local_orig_acc = 0.41\n",
    "local_ibdist_legit = 39.52875\n",
    "local_ibdist_adv = 49.74\n",
    "\n",
    "# Fedavg - Benign\n",
    "fedavg_adv_miss = 0.0\n",
    "fedavg_adv_target = 0.86 \n",
    "fedavg_orig_acc = 0.96\n",
    "fedavg_ibdist_legit = 0\n",
    "fedavg_ibdist_adv = 0\n",
    "\n",
    "# FedEM - Benign\n",
    "fedem_adv_miss = 0.12\n",
    "fedem_adv_target = 0.43\n",
    "fedem_orig_acc = 0.79\n",
    "fedem_ibdist_legit = 9.265\n",
    "fedem_ibdist_adv = 10.03\n",
    "\n",
    "# Local - adv\n",
    "local_adv_miss = 0.30\n",
    "local_adv_target = 0.05\n",
    "local_orig_acc = 0.30\n",
    "local_ibdist_legit = 50.21\n",
    "local_ibdist_adv = 87.08\n",
    "\n",
    "# Fedavg - adv\n",
    "fedavg_adv_miss = 0.352\n",
    "fedavg_adv_target =  0.21\n",
    "fedavg_orig_acc = 0.93\n",
    "fedavg_ibdist_legit = 0\n",
    "fedavg_ibdist_adv = 0\n",
    "\n",
    "# FedEM - adv\n",
    "fedem_adv_miss = 0.54\n",
    "fedem_adv_target = 0.073\n",
    "fedem_orig_acc = 0.71\n",
    "fedem_ibdist_legit = 14.29\n",
    "fedem_ibdist_adv = 48.97"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedEM_env",
   "language": "python",
   "name": "fedem_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
