{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Training\n",
    "\n",
    "TJ Kim\n",
    "\n",
    "11.29.21\n",
    "\n",
    "#### Summary:\n",
    "- Perform adversarial training in two different ways and analyze impact on inter-boundary distance of different variations of training\n",
    "- Method 1: Train 100 rounds of regular FedEM/FedLocal (Same seed) --> 100 Rounds of adversarial Training\n",
    "- Method 2: Train 200 rounds on benign model --> 200 rounds on adversarial model\n",
    "- Examinations 1: Benign --> Adv trained 1 and 2 (Transferability and boundary distance)\n",
    "- Examinations 2: Adv --> Adv trained 1 and 2 (\"\" \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/FedEM\n"
     ]
    }
   ],
   "source": [
    "cd /home/ubuntu/FedEM/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import General Libraries\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import FedEM based Libraries\n",
    "from utils.utils import *\n",
    "from utils.constants import *\n",
    "from utils.args import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from run_experiment import *\n",
    "from models import *\n",
    "\n",
    "# Import Transfer Attack\n",
    "from transfer_attacks.Personalized_NN import *\n",
    "from transfer_attacks.Params import *\n",
    "from transfer_attacks.Transferer import *\n",
    "from transfer_attacks.Args import *\n",
    "\n",
    "from transfer_attacks.Boundary_Transferer import *\n",
    "from transfer_attacks.projected_gradient_descent import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import FedEM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 80/80 [00:00<00:00, 177.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 80/80 [00:28<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Test Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++\n",
      "Global..\n",
      "Train Loss: 2.292 | Train Acc: 12.159% |Test Loss: 2.292 | Test Acc: 12.248% |\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "# Manually set argument parameters\n",
    "args_ = Args()\n",
    "args_.experiment = \"cifar10\"\n",
    "args_.method = \"FedEM\"\n",
    "args_.decentralized = False\n",
    "args_.sampling_rate = 1.0\n",
    "args_.input_dimension = None\n",
    "args_.output_dimension = None\n",
    "args_.n_learners= 3\n",
    "args_.n_rounds = 10\n",
    "args_.bz = 128\n",
    "args_.local_steps = 1\n",
    "args_.lr_lambda = 0\n",
    "args_.lr =0.03\n",
    "args_.lr_scheduler = 'multi_step'\n",
    "args_.log_freq = 10\n",
    "args_.device = 'cuda'\n",
    "args_.optimizer = 'sgd'\n",
    "args_.mu = 0\n",
    "args_.communication_probability = 0.1\n",
    "args_.q = 1\n",
    "args_.locally_tune_clients = False\n",
    "args_.seed = 1234\n",
    "args_.verbose = 1\n",
    "args_.save_path = 'weights/cifar/21_09_28_first_transfers/'\n",
    "args_.validation = False\n",
    "\n",
    "# Generate the dummy values here\n",
    "aggregator, clients = dummy_aggregator(args_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import weights for aggregator\n",
    "aggregator.load_state(args_.save_path)\n",
    "\n",
    "# This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "hypotheses = aggregator.global_learners_ensemble.learners\n",
    "\n",
    "# obtain the state dict for each of the weights \n",
    "weights_h = []\n",
    "\n",
    "for h in hypotheses:\n",
    "    weights_h += [h.model.state_dict()]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.load(\"weights/cifar/21_09_28_first_transfers/train_client_weights.npy\")\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "\n",
    "#print(weights)\n",
    "\n",
    "# Set model weights\n",
    "model_weights = []\n",
    "num_models = 7\n",
    "\n",
    "for i in range(num_models):\n",
    "    model_weights += [weights[i]]\n",
    "    \n",
    "    \n",
    "# Generate the weights to test on as linear combinations of the model_weights\n",
    "models_test = []\n",
    "\n",
    "for (w0,w1,w2) in model_weights:\n",
    "    # first make the model with empty weights\n",
    "    new_model = copy.deepcopy(hypotheses[0].model)\n",
    "    new_model.eval()\n",
    "    new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "    for key in weights_h[0]:\n",
    "        new_weight_dict[key] = w0*weights_h[0][key] + w1*weights_h[1][key] + w2*weights_h[2][key]\n",
    "    new_model.load_state_dict(new_weight_dict)\n",
    "    models_test += [new_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 2, 6, 0, 1, 4, 6, 1, 2, 9, 6, 1, 0, 1, 1, 6, 6, 5, 5, 5, 1, 5, 0, 4,\n",
       "        6, 9, 9, 2, 9, 5, 5, 1, 2, 5, 4, 2, 3, 6, 9, 6, 9, 6, 9, 6, 9, 1, 6, 2,\n",
       "        9, 9, 1, 1, 2, 1, 1, 4, 9, 5, 9, 1, 5, 1, 6, 2, 9, 1, 5, 5, 9, 2, 6, 1,\n",
       "        0, 0, 3, 4, 9, 1, 2, 4, 4, 4, 7, 6, 1, 1, 5, 4, 1, 2, 4, 4, 1, 6, 5, 4,\n",
       "        0, 2, 0, 0, 4, 2, 6, 6, 9, 9, 6, 5, 0, 4, 1, 4, 5, 2, 6, 4, 0, 9, 1, 5,\n",
       "        6, 0, 1, 6, 7, 8, 1, 4, 4, 4, 4, 6, 3, 6, 2, 9, 6, 0, 0, 5, 1, 0, 1, 9,\n",
       "        9, 2, 2, 4, 5, 2, 2, 9, 1, 5, 0, 1, 9, 2, 1, 2, 6, 1, 3, 2, 4, 5, 9, 6,\n",
       "        2, 0, 5, 4, 1, 2, 9, 0, 9, 0, 2, 5, 2, 9, 1, 4, 5, 0, 9, 9, 5, 0, 1, 5,\n",
       "        0, 9, 5, 4, 2, 1, 6, 2, 4, 9, 6, 1, 1, 9, 1, 9, 5, 2, 6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clients[0].train_iterator.dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregator.clients[0].train_iterator.dataset.data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 2, 6, 0, 1, 4, 6, 1, 2, 9, 6, 1, 0, 1, 1, 6, 6, 5, 5, 5, 1, 5, 0, 4,\n",
       "        6, 9, 9, 2, 9, 5, 5, 1, 2, 5, 4, 2, 3, 6, 9, 6, 9, 6, 9, 6, 9, 1, 6, 2,\n",
       "        9, 9, 1, 1, 2, 1, 1, 4, 9, 5, 9, 1, 5, 1, 6, 2, 9, 1, 5, 5, 9, 2, 6, 1,\n",
       "        0, 0, 3, 4, 9, 1, 2, 4, 4, 4, 7, 6, 1, 1, 5, 4, 1, 2, 4, 4, 1, 6, 5, 4,\n",
       "        0, 2, 0, 0, 4, 2, 6, 6, 9, 9, 6, 5, 0, 4, 1, 4, 5, 2, 6, 4, 0, 9, 1, 5,\n",
       "        6, 0, 1, 6, 7, 8, 1, 4, 4, 4, 4, 6, 3, 6, 2, 9, 6, 0, 0, 5, 1, 0, 1, 9,\n",
       "        9, 2, 2, 4, 5, 2, 2, 9, 1, 5, 0, 1, 9, 2, 1, 2, 6, 1, 3, 2, 4, 5, 9, 6,\n",
       "        2, 0, 5, 4, 1, 2, 9, 0, 9, 0, 2, 5, 2, 9, 1, 4, 5, 0, 9, 9, 5, 0, 1, 5,\n",
       "        0, 9, 5, 4, 2, 1, 6, 2, 4, 9, 6, 1, 1, 9, 1, 9, 5, 2, 6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregator.clients[0].train_iterator.dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Validation Data across all clients as test\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for i in range(len(clients)):\n",
    "    daniloader = clients[i].val_iterator\n",
    "    for (x,y,idx) in daniloader.dataset:\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "\n",
    "data_x = torch.stack(data_x)\n",
    "data_y = torch.stack(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader from validation dataset that allows for diverse batch size\n",
    "dataloader = Custom_Dataloader(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0824, 0.0667, 0.0745,  ..., 0.0510, 0.0431, 0.0627],\n",
       "         [0.0863, 0.0706, 0.0784,  ..., 0.0471, 0.0392, 0.0549],\n",
       "         [0.0980, 0.0863, 0.0863,  ..., 0.0471, 0.0431, 0.0549],\n",
       "         ...,\n",
       "         [0.1843, 0.1686, 0.2235,  ..., 0.1176, 0.1176, 0.1059],\n",
       "         [0.2000, 0.1922, 0.3255,  ..., 0.1216, 0.1216, 0.1059],\n",
       "         [0.1686, 0.2118, 0.4745,  ..., 0.1294, 0.1216, 0.1020]],\n",
       "\n",
       "        [[0.5647, 0.5686, 0.5882,  ..., 0.5765, 0.5765, 0.5804],\n",
       "         [0.5647, 0.5647, 0.5804,  ..., 0.5686, 0.5686, 0.5686],\n",
       "         [0.5686, 0.5765, 0.5843,  ..., 0.5686, 0.5725, 0.5725],\n",
       "         ...,\n",
       "         [0.6196, 0.5569, 0.5451,  ..., 0.5569, 0.6000, 0.6039],\n",
       "         [0.6431, 0.5961, 0.6039,  ..., 0.5412, 0.5961, 0.6078],\n",
       "         [0.6588, 0.6431, 0.6980,  ..., 0.5216, 0.5804, 0.5961]],\n",
       "\n",
       "        [[0.7294, 0.7255, 0.7412,  ..., 0.7294, 0.7255, 0.7373],\n",
       "         [0.7216, 0.7176, 0.7333,  ..., 0.7216, 0.7176, 0.7216],\n",
       "         [0.7255, 0.7255, 0.7333,  ..., 0.7216, 0.7216, 0.7255],\n",
       "         ...,\n",
       "         [0.7529, 0.6627, 0.6157,  ..., 0.6784, 0.7333, 0.7412],\n",
       "         [0.8039, 0.7059, 0.6471,  ..., 0.6667, 0.7255, 0.7373],\n",
       "         [0.8275, 0.7529, 0.7255,  ..., 0.6510, 0.7098, 0.7216]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = clients[0].train_iterator.dataset.data[0]\n",
    "img = Image.fromarray(img1.numpy())\n",
    "tt = ToTensor()\n",
    "tt(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 21, 144, 186],\n",
       "         [ 17, 145, 185],\n",
       "         [ 19, 150, 189],\n",
       "         ...,\n",
       "         [ 13, 147, 186],\n",
       "         [ 11, 147, 185],\n",
       "         [ 16, 148, 188]],\n",
       "\n",
       "        [[ 22, 144, 184],\n",
       "         [ 18, 144, 183],\n",
       "         [ 20, 148, 187],\n",
       "         ...,\n",
       "         [ 12, 145, 184],\n",
       "         [ 10, 145, 183],\n",
       "         [ 14, 145, 184]],\n",
       "\n",
       "        [[ 25, 145, 185],\n",
       "         [ 22, 147, 185],\n",
       "         [ 22, 149, 187],\n",
       "         ...,\n",
       "         [ 12, 145, 184],\n",
       "         [ 11, 146, 184],\n",
       "         [ 14, 146, 185]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 47, 158, 192],\n",
       "         [ 43, 142, 169],\n",
       "         [ 57, 139, 157],\n",
       "         ...,\n",
       "         [ 30, 142, 173],\n",
       "         [ 30, 153, 187],\n",
       "         [ 27, 154, 189]],\n",
       "\n",
       "        [[ 51, 164, 205],\n",
       "         [ 49, 152, 180],\n",
       "         [ 83, 154, 165],\n",
       "         ...,\n",
       "         [ 31, 138, 170],\n",
       "         [ 31, 152, 185],\n",
       "         [ 27, 155, 188]],\n",
       "\n",
       "        [[ 43, 168, 211],\n",
       "         [ 54, 164, 192],\n",
       "         [121, 178, 185],\n",
       "         ...,\n",
       "         [ 33, 133, 166],\n",
       "         [ 31, 148, 181],\n",
       "         [ 26, 152, 184]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "forward = Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
    "normed = clients[0].train_iterator.dataset.transform(img)\n",
    "\n",
    "torch.reshape(img1,[3,32,32])\n",
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.tensor([0.4914, 0.4822, 0.4465])\n",
    "std = torch.tensor([0.2023, 0.1994, 0.201])\n",
    "\n",
    "unnormalize = Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
    "a = unnormalize(normed)\n",
    "a = a.transpose(0,1)\n",
    "a = a.transpose(1,2)\n",
    "a = a * 255\n",
    "b = a.clone().detach().requires_grad_(True).type(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 20, 144, 186],\n",
       "         [ 17, 145, 185],\n",
       "         [ 18, 150, 189],\n",
       "         ...,\n",
       "         [ 12, 147, 186],\n",
       "         [ 10, 147, 185],\n",
       "         [ 15, 148, 188]],\n",
       "\n",
       "        [[ 21, 144, 184],\n",
       "         [ 17, 144, 183],\n",
       "         [ 19, 148, 187],\n",
       "         ...,\n",
       "         [ 11, 145, 184],\n",
       "         [  9, 145, 183],\n",
       "         [ 13, 145, 184]],\n",
       "\n",
       "        [[ 24, 145, 185],\n",
       "         [ 21, 147, 185],\n",
       "         [ 21, 149, 187],\n",
       "         ...,\n",
       "         [ 11, 145, 184],\n",
       "         [ 10, 146, 184],\n",
       "         [ 13, 146, 185]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 46, 158, 192],\n",
       "         [ 42, 142, 169],\n",
       "         [ 56, 139, 157],\n",
       "         ...,\n",
       "         [ 29, 142, 173],\n",
       "         [ 29, 153, 187],\n",
       "         [ 26, 154, 189]],\n",
       "\n",
       "        [[ 50, 164, 205],\n",
       "         [ 48, 152, 180],\n",
       "         [ 82, 154, 165],\n",
       "         ...,\n",
       "         [ 30, 138, 170],\n",
       "         [ 30, 152, 185],\n",
       "         [ 26, 155, 188]],\n",
       "\n",
       "        [[ 42, 168, 211],\n",
       "         [ 53, 164, 192],\n",
       "         [120, 178, 185],\n",
       "         ...,\n",
       "         [ 32, 133, 166],\n",
       "         [ 30, 148, 181],\n",
       "         [ 25, 152, 184]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_cifar10(normed):\n",
    "\n",
    "    mean = torch.tensor([0.4914, 0.4822, 0.4465])\n",
    "    std = torch.tensor([0.2023, 0.1994, 0.201])\n",
    "\n",
    "    unnormalize = Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
    "    a = unnormalize(normed)\n",
    "    a = a.transpose(0,1)\n",
    "    a = a.transpose(1,2)\n",
    "    a = a * 255\n",
    "    b = a.clone().detach().requires_grad_(True).type(torch.uint8)\n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([47965, 3, 32, 32])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_xn = []\n",
    "data_yn = []\n",
    "rr = 1000\n",
    "\n",
    "for i in range(rr):\n",
    "    x = data_x[i]\n",
    "    x_new = unnormalize_cifar10(x)\n",
    "    \n",
    "    data_xn.append(x_new)\n",
    "\n",
    "data_xn = torch.stack(data_xn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 2, 6, 0, 1, 4, 6, 1, 2, 9])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.y_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 21, 144, 186],\n",
       "          [ 17, 145, 185],\n",
       "          [ 19, 150, 189],\n",
       "          ...,\n",
       "          [ 13, 147, 186],\n",
       "          [ 11, 147, 185],\n",
       "          [ 16, 148, 188]],\n",
       "\n",
       "         [[ 22, 144, 184],\n",
       "          [ 18, 144, 183],\n",
       "          [ 20, 148, 187],\n",
       "          ...,\n",
       "          [ 12, 145, 184],\n",
       "          [ 10, 145, 183],\n",
       "          [ 14, 145, 184]],\n",
       "\n",
       "         [[ 25, 145, 185],\n",
       "          [ 22, 147, 185],\n",
       "          [ 22, 149, 187],\n",
       "          ...,\n",
       "          [ 12, 145, 184],\n",
       "          [ 11, 146, 184],\n",
       "          [ 14, 146, 185]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 47, 158, 192],\n",
       "          [ 43, 142, 169],\n",
       "          [ 57, 139, 157],\n",
       "          ...,\n",
       "          [ 30, 142, 173],\n",
       "          [ 30, 153, 187],\n",
       "          [ 27, 154, 189]],\n",
       "\n",
       "         [[ 51, 164, 205],\n",
       "          [ 49, 152, 180],\n",
       "          [ 83, 154, 165],\n",
       "          ...,\n",
       "          [ 31, 138, 170],\n",
       "          [ 31, 152, 185],\n",
       "          [ 27, 155, 188]],\n",
       "\n",
       "         [[ 43, 168, 211],\n",
       "          [ 54, 164, 192],\n",
       "          [121, 178, 185],\n",
       "          ...,\n",
       "          [ 33, 133, 166],\n",
       "          [ 31, 148, 181],\n",
       "          [ 26, 152, 184]]],\n",
       "\n",
       "\n",
       "        [[[255, 152,   0],\n",
       "          [255, 150,   2],\n",
       "          [255, 150,   1],\n",
       "          ...,\n",
       "          [255, 150,   0],\n",
       "          [255, 150,   0],\n",
       "          [255, 150,   0]],\n",
       "\n",
       "         [[255, 149,   0],\n",
       "          [254, 147,   3],\n",
       "          [252, 145,   0],\n",
       "          ...,\n",
       "          [254, 148,   0],\n",
       "          [254, 148,   0],\n",
       "          [254, 148,   0]],\n",
       "\n",
       "         [[255, 150,   1],\n",
       "          [254, 146,   2],\n",
       "          [248, 157,  28],\n",
       "          ...,\n",
       "          [255, 148,   1],\n",
       "          [255, 148,   0],\n",
       "          [255, 148,   0]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[254, 150,   4],\n",
       "          [204, 147,  49],\n",
       "          [173, 142,  78],\n",
       "          ...,\n",
       "          [255, 148,   0],\n",
       "          [255, 148,   0],\n",
       "          [255, 148,   0]],\n",
       "\n",
       "         [[255, 150,   0],\n",
       "          [228, 145,  20],\n",
       "          [179, 142,  77],\n",
       "          ...,\n",
       "          [255, 148,   0],\n",
       "          [255, 148,   0],\n",
       "          [255, 148,   0]],\n",
       "\n",
       "         [[255, 150,   0],\n",
       "          [250, 147,   3],\n",
       "          [216, 150,  58],\n",
       "          ...,\n",
       "          [255, 148,   0],\n",
       "          [255, 148,   0],\n",
       "          [255, 148,   0]]],\n",
       "\n",
       "\n",
       "        [[[186, 185, 149],\n",
       "          [174, 182, 134],\n",
       "          [173, 183, 136],\n",
       "          ...,\n",
       "          [174, 143, 136],\n",
       "          [172, 141, 139],\n",
       "          [168, 146, 141]],\n",
       "\n",
       "         [[188, 184, 148],\n",
       "          [184, 187, 142],\n",
       "          [175, 181, 135],\n",
       "          ...,\n",
       "          [129,  99,  87],\n",
       "          [155, 132, 125],\n",
       "          [167, 153, 139]],\n",
       "\n",
       "         [[200, 193, 159],\n",
       "          [201, 199, 159],\n",
       "          [188, 190, 147],\n",
       "          ...,\n",
       "          [140, 132, 107],\n",
       "          [152, 153, 124],\n",
       "          [171, 174, 143]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[164, 140, 127],\n",
       "          [160, 137, 125],\n",
       "          [200, 178, 165],\n",
       "          ...,\n",
       "          [150, 135, 139],\n",
       "          [154, 136, 136],\n",
       "          [198, 177, 175]],\n",
       "\n",
       "         [[200, 184, 170],\n",
       "          [166, 150, 136],\n",
       "          [155, 139, 125],\n",
       "          ...,\n",
       "          [131, 121, 129],\n",
       "          [136, 128, 132],\n",
       "          [191, 183, 181]],\n",
       "\n",
       "         [[201, 185, 173],\n",
       "          [174, 159, 146],\n",
       "          [151, 135, 123],\n",
       "          ...,\n",
       "          [102,  94, 105],\n",
       "          [113, 109, 115],\n",
       "          [168, 165, 164]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[233, 219, 176],\n",
       "          [232, 219, 176],\n",
       "          [233, 219, 177],\n",
       "          ...,\n",
       "          [167, 158, 114],\n",
       "          [124, 119,  89],\n",
       "          [113, 110,  81]],\n",
       "\n",
       "         [[233, 218, 173],\n",
       "          [235, 221, 176],\n",
       "          [234, 220, 177],\n",
       "          ...,\n",
       "          [198, 185, 144],\n",
       "          [136, 130,  95],\n",
       "          [110, 105,  78]],\n",
       "\n",
       "         [[226, 212, 165],\n",
       "          [229, 214, 171],\n",
       "          [230, 217, 175],\n",
       "          ...,\n",
       "          [139, 126,  98],\n",
       "          [109,  98,  76],\n",
       "          [101,  92,  74]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[200, 185, 135],\n",
       "          [201, 184, 135],\n",
       "          [194, 178, 128],\n",
       "          ...,\n",
       "          [191, 170, 119],\n",
       "          [171, 143,  92],\n",
       "          [203, 177, 122]],\n",
       "\n",
       "         [[205, 188, 135],\n",
       "          [193, 178, 127],\n",
       "          [136, 126,  93],\n",
       "          ...,\n",
       "          [198, 176, 124],\n",
       "          [174, 147,  95],\n",
       "          [216, 193, 136]],\n",
       "\n",
       "         [[195, 179, 125],\n",
       "          [144, 135,  94],\n",
       "          [ 79,  77,  59],\n",
       "          ...,\n",
       "          [193, 171, 123],\n",
       "          [176, 147,  92],\n",
       "          [212, 185, 126]]],\n",
       "\n",
       "\n",
       "        [[[193, 207, 209],\n",
       "          [192, 206, 207],\n",
       "          [193, 207, 208],\n",
       "          ...,\n",
       "          [199, 209, 213],\n",
       "          [198, 209, 213],\n",
       "          [199, 209, 213]],\n",
       "\n",
       "         [[196, 210, 211],\n",
       "          [195, 209, 210],\n",
       "          [196, 210, 211],\n",
       "          ...,\n",
       "          [201, 212, 216],\n",
       "          [201, 212, 216],\n",
       "          [201, 212, 216]],\n",
       "\n",
       "         [[194, 208, 209],\n",
       "          [193, 207, 208],\n",
       "          [194, 208, 209],\n",
       "          ...,\n",
       "          [200, 210, 214],\n",
       "          [200, 210, 214],\n",
       "          [200, 210, 214]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[178, 192, 189],\n",
       "          [189, 204, 204],\n",
       "          [192, 206, 207],\n",
       "          ...,\n",
       "          [159, 165, 152],\n",
       "          [192, 204, 209],\n",
       "          [194, 208, 211]],\n",
       "\n",
       "         [[177, 191, 188],\n",
       "          [189, 203, 203],\n",
       "          [192, 205, 206],\n",
       "          ...,\n",
       "          [154, 161, 150],\n",
       "          [192, 205, 210],\n",
       "          [194, 209, 212]],\n",
       "\n",
       "         [[177, 190, 187],\n",
       "          [189, 203, 203],\n",
       "          [191, 205, 206],\n",
       "          ...,\n",
       "          [156, 163, 156],\n",
       "          [193, 205, 210],\n",
       "          [195, 209, 212]]],\n",
       "\n",
       "\n",
       "        [[[255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255]],\n",
       "\n",
       "         [[255, 255, 255],\n",
       "          [254, 254, 254],\n",
       "          [255, 255, 255],\n",
       "          ...,\n",
       "          [254, 254, 254],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255]],\n",
       "\n",
       "         [[255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          ...,\n",
       "          [254, 254, 254],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[255, 255, 255],\n",
       "          [253, 254, 255],\n",
       "          [254, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255]],\n",
       "\n",
       "         [[255, 255, 255],\n",
       "          [252, 254, 253],\n",
       "          [251, 255, 254],\n",
       "          ...,\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255]],\n",
       "\n",
       "         [[255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clients[0].train_iterator.dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedEM_env",
   "language": "python",
   "name": "fedem_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
