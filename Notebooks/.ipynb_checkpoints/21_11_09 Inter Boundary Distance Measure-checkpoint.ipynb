{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inter-Boundary Distance Measurement\n",
    "TJ Kim\n",
    "\n",
    "11.9.21\n",
    "\n",
    "### Summary:\n",
    "- Given multiple models performing identical task and a test set, measure the inter-boundary distance to measure transferability between models.\n",
    "- Legitimate direction - any point x and closest data point in test set that changes the class\n",
    "- Adversarial direction - any point x and smallest amount of perturbation that changes class\n",
    "- Random direction - any point x and random perturbation drawn uniformly to misclassify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/FedEM\n"
     ]
    }
   ],
   "source": [
    "cd /home/ubuntu/FedEM/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import General Libraries\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import FedEM based Libraries\n",
    "from utils.utils import *\n",
    "from utils.constants import *\n",
    "from utils.args import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from run_experiment import *\n",
    "from models import *\n",
    "\n",
    "# Import Transfer Attack\n",
    "from transfer_attacks.Personalized_NN import *\n",
    "from transfer_attacks.Params import *\n",
    "from transfer_attacks.Transferer import *\n",
    "from transfer_attacks.Args import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boundary Transferer Imports\n",
    "# Import Custom Made Victim\n",
    "from transfer_attacks.Personalized_NN import *\n",
    "from transfer_attacks.Params import *\n",
    "from transfer_attacks.Attack_Metrics import *\n",
    "import pandas as pd\n",
    "from transfer_attacks.Transferer import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boundary Transferer Class\n",
    "- Used to calcualte direction of all 3 types\n",
    "- Based off of (not subclass of) the Transferer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Boundary_Transferer(): \n",
    "    \"\"\"\n",
    "    - Load all the datasets but separate them\n",
    "    - Intermediate values of featues after 2 convolution layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models_list, dataloader):\n",
    "        \n",
    "        self.models_list = models_list\n",
    "        self.dataloader = dataloader\n",
    "        \n",
    "        self.advNN_idx = None\n",
    "        self.advNN = None # Dictionary to hold adversary \n",
    "        self.atk_order = None\n",
    "        \n",
    "        self.base_nn_idx = None\n",
    "        self.victim_idx = None\n",
    "        \n",
    "        self.fixed_point = None # {\"idx\",\"x\",\"y\"}\n",
    "        self.comparison_set = None # For legitimate direction\n",
    "        self.comparison_x = None\n",
    "        self.comparison_y = None\n",
    "        \n",
    "        # Target that is the closest point in legit used in adv targeted attack\n",
    "        self.y_comparison = None\n",
    "        \n",
    "        # Put single model from model_list into adversarial NN (can perform attacks)\n",
    "        adv_nn = None\n",
    "        self.atk_params = IFSGM_Params()\n",
    "        \n",
    "    def set_adv_NN(self, idx):\n",
    "        \n",
    "        self.advNN = copy.deepcopy(Adv_NN(self.models_list[idx], self.dataloader))\n",
    "        \n",
    "    def set_ensemble_adv_NN(self, client_idx):\n",
    "        \n",
    "        self.advNN_idx = client_idx # List\n",
    "        self.advNN = {} # Dict of NN\n",
    "        \n",
    "        for i in client_idx:\n",
    "            self.advNN[i] = copy.deepcopy(Adv_NN(self.models_list[i], self.dataloader))\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def select_data_point(self, explore_set_size = 100):\n",
    "        \"\"\"\n",
    "        Select a single data point to use as comparison of different boundary types\n",
    "        Change - all members of the system must classify this point the same \n",
    "        \"\"\"\n",
    "        self.fixed_point = {}\n",
    "        found_point_flag = False\n",
    "        \n",
    "        while not found_point_flag:\n",
    "            x, y = self.dataloader.load_batch(explore_set_size)\n",
    "            correct_idx = {}\n",
    "\n",
    "            # Analyze Dataset and find point that is classified correctly\n",
    "            for nidx in range(len(self.models_list)):\n",
    "                preds = torch.argmax(self.models_list[nidx](x),axis=1)\n",
    "                correct_idx[nidx] = torch.where(preds == y)[0]\n",
    "\n",
    "            temp_idx = correct_idx[0]\n",
    "\n",
    "            for nidx in range(1,len(self.models_list)):\n",
    "                comp_idx = correct_idx[nidx]\n",
    "                indices = torch.zeros_like(temp_idx,dtype=torch.bool,device='cuda')\n",
    "                for elem in comp_idx:\n",
    "                    indices = indices | (temp_idx==elem)\n",
    "                temp_idx = temp_idx[indices]\n",
    "\n",
    "            # Select a point within remaining at random\n",
    "            # print(temp_idx.numel())\n",
    "            if not temp_idx.numel():\n",
    "                continue\n",
    "            else:\n",
    "                chosen_idx = temp_idx[np.random.randint(temp_idx.shape[0],size=1)[0]]\n",
    "                self.fixed_point[\"x\"] = x[chosen_idx]\n",
    "                self.fixed_point[\"y\"] = y[chosen_idx]\n",
    "                found_point_flag = True\n",
    "            \n",
    "        return \n",
    "    \n",
    "    def select_comparison_set(self,batch_size):\n",
    "        \"\"\"\n",
    "        Select multiple datapoints to use to compare \n",
    "        \"\"\"\n",
    "        \n",
    "        xs, ys = self.dataloader.load_batch(batch_size)\n",
    "        \n",
    "        self.comparison_set = {}\n",
    "        \n",
    "        self.comparison_set[\"x\"] = xs\n",
    "        self.comparison_set[\"y\"] = ys\n",
    "        \n",
    "        # Eliminate \n",
    "        \n",
    "        return\n",
    "    \n",
    "    def measure_distance(self, x1, x2_set):\n",
    "        \n",
    "        x2_dist = torch.subtract(x2_set, x1)\n",
    "        x2_l2 = torch.linalg.norm(x2_dist.flatten(start_dim=1),ord = 2, dim=1)\n",
    "        \n",
    "        return x2_dist, x2_l2\n",
    "    \n",
    "    def ensemble_attack_order(self):\n",
    "        \n",
    "        num_iters = self.atk_params.iteration\n",
    "        atk_order = []\n",
    "        \n",
    "        for t in range(num_iters):\n",
    "            idx = t%len(self.advNN_idx)\n",
    "            atk_order += [self.advNN_idx[idx]]\n",
    "            \n",
    "        self.atk_order = atk_order\n",
    "    \n",
    "    def sweep_victim_distance(self, og_ep, min_dist_unit_vector, ep_granularity, rep_padding, print_res = False):\n",
    "        \"\"\"\n",
    "        Given direction of misclassification, calculate discance needed to cross boundary of victims\n",
    "        \"\"\"\n",
    "        \n",
    "        # Sweep epilson in unit direction for all NN\n",
    "        num_sweep = int(np.ceil(og_ep)/ep_granularity)\n",
    "        # Base NN\n",
    "        base_ep = 0\n",
    "        for e in range(1,num_sweep+1):\n",
    "            \n",
    "            delta = (min_dist_unit_vector * e * ep_granularity).unsqueeze(0)\n",
    "            x_in = self.fixed_point[\"x\"] + delta\n",
    "            \n",
    "            y_dist = self.models_list[self.base_nn_idx](x_in)\n",
    "\n",
    "            y_idx = torch.argmax(y_dist, axis=1)\n",
    "            \n",
    "            if y_idx != self.fixed_point[\"y\"]:\n",
    "                if print_res:\n",
    "                    print(\"og distance:\", og_ep)\n",
    "                    print(\"iteration:\",e,\", ep:\", e * ep_granularity)\n",
    "                    print(\"true y:\", self.fixed_point[\"y\"])\n",
    "                    print(\"pred_y:\", y_idx)\n",
    "                    print(\"relative_y:\",self.comparison_y[min_dist_idx])\n",
    "                    scaled_y = self.models_list[self.base_nn_idx](self.fixed_point[\"x\"] \n",
    "                                + (min_dist_unit_vector * num_sweep * ep_granularity).unsqueeze(0))\n",
    "                    scaled_y = torch.argmax(scaled_y,axis=1)\n",
    "                    print(\"scaled_ep_y:\",scaled_y)\n",
    "                base_ep = e*ep_granularity\n",
    "                break\n",
    "            \n",
    "        # Victim NN\n",
    "        victim_eps = {}\n",
    "        num_sweep_v = num_sweep + rep_padding\n",
    "        for v_idx in self.victim_idx:\n",
    "            for e in range(1, num_sweep_v+1):\n",
    "                delta = (min_dist_unit_vector * e * ep_granularity).unsqueeze(0)\n",
    "                x_in = self.fixed_point[\"x\"] + delta\n",
    "\n",
    "                y_dist = self.models_list[v_idx](x_in)\n",
    "                y_idx = torch.argmax(y_dist, axis=1)\n",
    "\n",
    "                if y_idx != self.fixed_point[\"y\"]:\n",
    "                    victim_eps[v_idx] = e*ep_granularity\n",
    "                    break\n",
    "                                                                          \n",
    "        return base_ep, victim_eps\n",
    "    \n",
    "    def legitimate_direction(self, batch_size, ep_granularity = 0.5, rep_padding = 1000, new_point = True,\n",
    "                            print_res = False):\n",
    "        \"\"\"\n",
    "        Calculate Legitimate Direction for a single point\n",
    "        \"\"\"\n",
    "        \n",
    "        # Select point of baseline comparison \n",
    "        if new_point:\n",
    "            self.select_data_point()\n",
    "        \n",
    "        # Select set of comparison \n",
    "        self.select_comparison_set(batch_size)\n",
    "        \n",
    "        # Calculate X distance\n",
    "        x_dists, x_dists_l2 = self.measure_distance(self.fixed_point[\"x\"], self.comparison_set[\"x\"])\n",
    "        \n",
    "        # Classify all members of comparison set\n",
    "        y_pred_nn = None\n",
    "        min_dist_idx = None\n",
    "        min_dist_unit_vector = None\n",
    "        \n",
    "        # Classify each data for each classifier\n",
    "        temp_classified = self.models_list[self.base_nn_idx](self.comparison_set[\"x\"])\n",
    "        y_pred_nn = torch.argmax(temp_classified,axis=1)\n",
    "\n",
    "        # Filter twice - argmin (distance), conditioned on different label\n",
    "        dist_mask = torch.where(y_pred_nn != self.fixed_point[\"y\"], x_dists_l2, torch.max(x_dists_l2))\n",
    "        min_dist_idx = torch.argmin(dist_mask)\n",
    "        min_dist_unit_vector = torch.divide(x_dists[min_dist_idx], \n",
    "                                                torch.linalg.norm(x_dists[min_dist_idx].flatten(),ord=2))\n",
    "        \n",
    "        og_ep = torch.linalg.norm(x_dists[min_dist_idx].flatten(),ord=2).data.tolist()\n",
    "            \n",
    "        self.y_comparison = self.comparison_set[\"y\"][min_dist_idx]\n",
    "        \n",
    "        base_ep, victim_eps = self.sweep_victim_distance(og_ep, min_dist_unit_vector, \n",
    "                                                         ep_granularity, rep_padding, print_res = False)\n",
    "                                                                          \n",
    "        return base_ep, victim_eps\n",
    "    \n",
    "    def adversarial_direction(self, ep_granularity = 0.5, rep_padding = 1000, new_point = True, \n",
    "                              print_res = False, target = True):\n",
    "        \"\"\"\n",
    "        Calculate adversarial direction for a single point (same as before?) \n",
    "        Return base_epsilon for comparison model, and victim_eps for others\n",
    "        \"\"\"\n",
    "        \n",
    "        # Select point of baseline comparison \n",
    "        if new_point:\n",
    "            self.select_data_point()\n",
    "        \n",
    "        # Perform a single step of attack on data point \n",
    "        x_in = self.fixed_point[\"x\"]\n",
    "        y_in = self.fixed_point[\"y\"]\n",
    "        \n",
    "        if target and (self.y_comparison is not None):\n",
    "            self.atk_params.target = self.y_comparison\n",
    "        \n",
    "        self.advNN.i_fgsm_sub(self.atk_params,x_in.unsqueeze(0),y_in.unsqueeze(0))\n",
    "        \n",
    "        x_adv = self.advNN.x_adv\n",
    "        dist_diff = torch.subtract(x_adv, x_in)\n",
    "        \n",
    "        min_dist_unit_vector = torch.divide(dist_diff, torch.linalg.norm(dist_diff.flatten(),ord=2))[0]\n",
    "        og_ep = torch.linalg.norm(dist_diff.flatten(),ord=2).data.tolist()\n",
    "        \n",
    "        base_ep, victim_eps = self.sweep_victim_distance(og_ep, min_dist_unit_vector, \n",
    "                                                         ep_granularity, rep_padding, print_res = False)\n",
    "                                                                                                                                                    \n",
    "        return base_ep, victim_eps\n",
    "    \n",
    "    def ensemble_adversarial_direction(self, ep_granularity = 0.5, rep_padding = 1000, new_point = True, \n",
    "                              print_res = False, target = True):\n",
    "        \n",
    "        # Select point of baseline comparison \n",
    "        if new_point:\n",
    "            self.select_data_point()\n",
    "        \n",
    "        # Perform a single step of attack on data point \n",
    "        x_in = self.fixed_point[\"x\"]\n",
    "        y_in = self.fixed_point[\"y\"]\n",
    "        \n",
    "        adv_x_in = x_in.unsqueeze(0)\n",
    "        adv_y_in = y_in.unsqueeze(0)\n",
    "        \n",
    "        if target and (self.y_comparison is not None):\n",
    "            self.atk_params.target = self.y_comparison\n",
    "            \n",
    "        # Decide on attack sequence\n",
    "        self.ensemble_attack_order()\n",
    "        \n",
    "        # Alter number of iteration in params to 1 \n",
    "        temp_params = copy.deepcopy(self.atk_params)\n",
    "        temp_params.iteration = 1\n",
    "        \n",
    "        for idx in self.atk_order:\n",
    "            self.advNN[idx].i_fgsm_sub(temp_params, adv_x_in, adv_y_in)\n",
    "            adv_x_in = copy.deepcopy(self.advNN[idx].x_adv)\n",
    "        \n",
    "        # Record relevant tensors\n",
    "        x_adv = copy.deepcopy(adv_x_in).cuda()\n",
    "        \n",
    "        dist_diff = torch.subtract(x_adv, x_in)\n",
    "        \n",
    "        min_dist_unit_vector = torch.divide(dist_diff, torch.linalg.norm(dist_diff.flatten(),ord=2))[0]\n",
    "        og_ep = torch.linalg.norm(dist_diff.flatten(),ord=2).data.tolist()\n",
    "        \n",
    "        base_ep, victim_eps = self.sweep_victim_distance(og_ep, min_dist_unit_vector, \n",
    "                                                         ep_granularity, rep_padding, print_res = False)\n",
    "                                                                                                                                                    \n",
    "        return base_ep, victim_eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legitimate Direction Calculation - FedEM\n",
    "- Across test set perform classification for each of the nueral networks \n",
    "- Measure distance from a single point to all other points \n",
    "- Find the closest point with different label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 80/80 [00:00<00:00, 221.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 80/80 [00:31<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Test Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++\n",
      "Global..\n",
      "Train Loss: 2.292 | Train Acc: 12.159% |Test Loss: 2.292 | Test Acc: 12.248% |\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "# Manually set argument parameters\n",
    "args_ = Args()\n",
    "args_.experiment = \"cifar10\"\n",
    "args_.method = \"FedEM\"\n",
    "args_.decentralized = False\n",
    "args_.sampling_rate = 1.0\n",
    "args_.input_dimension = None\n",
    "args_.output_dimension = None\n",
    "args_.n_learners= 3\n",
    "args_.n_rounds = 10\n",
    "args_.bz = 128\n",
    "args_.local_steps = 1\n",
    "args_.lr_lambda = 0\n",
    "args_.lr =0.03\n",
    "args_.lr_scheduler = 'multi_step'\n",
    "args_.log_freq = 10\n",
    "args_.device = 'cuda'\n",
    "args_.optimizer = 'sgd'\n",
    "args_.mu = 0\n",
    "args_.communication_probability = 0.1\n",
    "args_.q = 1\n",
    "args_.locally_tune_clients = False\n",
    "args_.seed = 1234\n",
    "args_.verbose = 1\n",
    "args_.save_path = 'weights/cifar/21_09_28_first_transfers/'\n",
    "args_.validation = False\n",
    "\n",
    "# Generate the dummy values here\n",
    "aggregator, clients = dummy_aggregator(args_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import weights for aggregator\n",
    "aggregator.load_state(args_.save_path)\n",
    "\n",
    "# This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "hypotheses = aggregator.global_learners_ensemble.learners\n",
    "\n",
    "# obtain the state dict for each of the weights \n",
    "weights_h = []\n",
    "\n",
    "for h in hypotheses:\n",
    "    weights_h += [h.model.state_dict()]\n",
    "    \n",
    "weights = np.load(\"weights/cifar/21_09_28_first_transfers/train_client_weights.npy\")\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "\n",
    "#print(weights)\n",
    "\n",
    "# Set model weights\n",
    "model_weights = []\n",
    "num_models = 7\n",
    "\n",
    "for i in range(num_models):\n",
    "    model_weights += [weights[i]]\n",
    "    \n",
    "    \n",
    "# Generate the weights to test on as linear combinations of the model_weights\n",
    "models_test = []\n",
    "\n",
    "for (w0,w1,w2) in model_weights:\n",
    "    # first make the model with empty weights\n",
    "    new_model = copy.deepcopy(hypotheses[0].model)\n",
    "    new_model.eval()\n",
    "    new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "    for key in weights_h[0]:\n",
    "        new_weight_dict[key] = w0*weights_h[0][key] + w1*weights_h[1][key] + w2*weights_h[2][key]\n",
    "    new_model.load_state_dict(new_weight_dict)\n",
    "    models_test += [new_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Validation Data across all clients as test\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for i in range(len(clients)):\n",
    "    daniloader = clients[i].val_iterator\n",
    "    for (x,y,idx) in daniloader.dataset:\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "\n",
    "data_x = torch.stack(data_x)\n",
    "data_y = torch.stack(data_y)\n",
    "\n",
    "# Create dataloader from validation dataset that allows for diverse batch size\n",
    "dataloader = Custom_Dataloader(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 200\n",
    "batch_size = 5000\n",
    "adv_idx = [0,1]\n",
    "\n",
    "t1 = Boundary_Transferer(models_list=models_test, dataloader=dataloader)\n",
    "t1.base_nn_idx = 0\n",
    "t1.victim_idx = [1,2,3,4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_551890/2702331145.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdists_measure_legit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvictim_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdists_measure_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvictim_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdists_measure_adv_ensemble\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvictim_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 't1' is not defined"
     ]
    }
   ],
   "source": [
    "dists_measure_legit = np.zeros([num_trials, len(t1.victim_idx)])\n",
    "dists_measure_adv = np.zeros([num_trials, len(t1.victim_idx)])\n",
    "dists_measure_adv_ensemble = np.zeros([num_trials, len(t1.victim_idx)])\n",
    "\n",
    "for i in range(num_trials):\n",
    "    print(\"num_trial:\", i)\n",
    "    t1 = Boundary_Transferer(models_list=models_test, dataloader=dataloader)\n",
    "    t1.base_nn_idx = 0\n",
    "    t1.victim_idx = [1,2,3,4]\n",
    "\n",
    "    t1.atk_params.set_params(batch_size=1, eps=0.1, alpha=0.01, iteration = 30,\n",
    "                       target = -1, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x))\n",
    "    t1.set_adv_NN(t1.base_nn_idx)\n",
    "\n",
    "    base_ep_legit, victim_eps_legit = t1.legitimate_direction(batch_size=batch_size, ep_granularity = 0.5, \n",
    "                                                              rep_padding = 1000, new_point = True,print_res = False)\n",
    "    \n",
    "    base_ep_adv, victim_eps_adv = t1.adversarial_direction(ep_granularity = 0.5, \n",
    "                                                              rep_padding = 1000, new_point = False,print_res = False)\n",
    "    \n",
    "    t1.set_ensemble_adv_NN(adv_idx)\n",
    "    base_ep_Eadv, victim_eps_Eadv = t1.ensemble_adversarial_direction(ep_granularity = 0.5, \n",
    "                                                              rep_padding = 1000, new_point = False,print_res = False)\n",
    "    \n",
    "    idx = 0\n",
    "    for key, value in victim_eps_legit.items():\n",
    "        dists_measure_legit[i,idx] = np.abs(base_ep_legit-value)\n",
    "        idx+=1\n",
    "        \n",
    "    idx = 0\n",
    "    for key, value in victim_eps_adv.items():\n",
    "        dists_measure_adv[i,idx] = np.abs(base_ep_adv - value)\n",
    "        idx+=1\n",
    "    \n",
    "    idx = 0\n",
    "    for key, value in victim_eps_Eadv.items():\n",
    "        dists_measure_adv_ensemble[i,idx] = np.abs(base_ep_Eadv - value)\n",
    "        idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(dists_measure_legit,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(dists_measure_adv,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(dists_measure_adv_ensemble,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legitimate Direction Calculation - Local\n",
    "- Across test set perform classification for each of the nueral networks \n",
    "- Measure distance from a single point to all other points \n",
    "- Find the closest point with different label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set argument parameters\n",
    "args_ = Args()\n",
    "args_.experiment = \"cifar10\"\n",
    "args_.method = \"local\"\n",
    "args_.decentralized = False\n",
    "args_.sampling_rate = 1.0\n",
    "args_.input_dimension = None\n",
    "args_.output_dimension = None\n",
    "args_.n_learners= 3\n",
    "args_.n_rounds = 10\n",
    "args_.bz = 128\n",
    "args_.local_steps = 1\n",
    "args_.lr_lambda = 0\n",
    "args_.lr =0.03\n",
    "args_.lr_scheduler = 'multi_step'\n",
    "args_.log_freq = 10\n",
    "args_.device = 'cuda'\n",
    "args_.optimizer = 'sgd'\n",
    "args_.mu = 0\n",
    "args_.communication_probability = 0.1\n",
    "args_.q = 1\n",
    "args_.locally_tune_clients = False\n",
    "args_.seed = 1234\n",
    "args_.verbose = 1\n",
    "args_.save_path = 'weights/cifar/21_11_06_local/'\n",
    "args_.validation = False\n",
    "\n",
    "# Generate the dummy values here\n",
    "aggregator, clients = dummy_aggregator(args_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import weights for aggregator\n",
    "aggregator.load_state(args_.save_path)\n",
    "\n",
    "# This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "hypotheses = aggregator.global_learners_ensemble.learners\n",
    "\n",
    "# obtain the state dict for each of the weights \n",
    "weights_h = []\n",
    "\n",
    "for h in hypotheses:\n",
    "    weights_h += [h.model.state_dict()]\n",
    "    \n",
    "weights = np.load(\"weights/cifar/21_09_28_first_transfers/train_client_weights.npy\")\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "\n",
    "#print(weights)\n",
    "\n",
    "# Set model weights\n",
    "model_weights = []\n",
    "num_models = 7\n",
    "\n",
    "for i in range(num_models):\n",
    "    model_weights += [weights[i]]\n",
    "    \n",
    "    \n",
    "# Generate the weights to test on as linear combinations of the model_weights\n",
    "models_test = []\n",
    "\n",
    "for (w0,w1,w2) in model_weights:\n",
    "    # first make the model with empty weights\n",
    "    new_model = copy.deepcopy(hypotheses[0].model)\n",
    "    new_model.eval()\n",
    "    new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "    for key in weights_h[0]:\n",
    "        new_weight_dict[key] = w0*weights_h[0][key] + w1*weights_h[1][key] + w2*weights_h[2][key]\n",
    "    new_model.load_state_dict(new_weight_dict)\n",
    "    models_test += [new_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 200\n",
    "batch_size = 5000\n",
    "adv_idx = [0,1]\n",
    "\n",
    "t1 = Boundary_Transferer(models_list=models_test, dataloader=dataloader)\n",
    "t1.base_nn_idx = 0\n",
    "t1.victim_idx = [1,2,3,4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_measure_legit = np.zeros([num_trials, len(t1.victim_idx)])\n",
    "dists_measure_adv = np.zeros([num_trials, len(t1.victim_idx)])\n",
    "dists_measure_adv_ensemble = np.zeros([num_trials, len(t1.victim_idx)])\n",
    "\n",
    "for i in range(num_trials):\n",
    "    print(\"num_trial:\", i)\n",
    "    \n",
    "    t1 = Boundary_Transferer(models_list=models_test, dataloader=dataloader)\n",
    "    t1.base_nn_idx = 0\n",
    "    t1.victim_idx = [1,2,3,4]\n",
    "\n",
    "    t1.atk_params.set_params(batch_size=1, eps=0.1, alpha=0.01, iteration = 30,\n",
    "                       target = -1, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x))\n",
    "    t1.set_adv_NN(t1.base_nn_idx)\n",
    "    \n",
    "    base_ep_legit, victim_eps_legit = t1.legitimate_direction(batch_size=batch_size, ep_granularity = 0.5, \n",
    "                                                              rep_padding = 1000, new_point = True,print_res = False)\n",
    "    \n",
    "    base_ep_adv, victim_eps_adv = t1.adversarial_direction(ep_granularity = 0.5, \n",
    "                                                              rep_padding = 1000, new_point = False,print_res = False)\n",
    "    \n",
    "    t1.set_ensemble_adv_NN(adv_idx)\n",
    "    base_ep_Eadv, victim_eps_Eadv = t1.ensemble_adversarial_direction(ep_granularity = 0.5, \n",
    "                                                              rep_padding = 1000, new_point = False,print_res = False)\n",
    "    \n",
    "    idx = 0\n",
    "    for key, value in victim_eps_legit.items():\n",
    "        dists_measure_legit[i,idx] = np.abs(base_ep_legit-value)\n",
    "        idx+=1\n",
    "        \n",
    "    idx = 0\n",
    "    for key, value in victim_eps_adv.items():\n",
    "        dists_measure_adv[i,idx] = np.abs(base_ep_adv - value)\n",
    "        idx+=1\n",
    "    \n",
    "    idx = 0\n",
    "    for key, value in victim_eps_Eadv.items():\n",
    "        dists_measure_adv_ensemble[i,idx] = np.abs(base_ep_Eadv - value)\n",
    "        idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(dists_measure_legit,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(dists_measure_adv,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(dists_measure_adv_ensemble,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedEM_env",
   "language": "python",
   "name": "fedem_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
